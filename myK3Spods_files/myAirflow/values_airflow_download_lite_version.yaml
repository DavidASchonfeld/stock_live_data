# Airflow Type Executor

executor: LocalExecutor


# -------- Airflow Environment Variables -------- 

env:
 - name: AIRFLOW__CORE__LOAD_EXAMPLES
   value: "False"


env:
 - name: AIRFLOW__CORE__DAGS_FOLDER
   value: /opt/airflow/dags  
   # The folder that airflow looks at to find dags aka task flows
   # This (/opt/airflow/dags) is already the default value for AIRFLOW__CORE__DAGS_FOLDER
   # I'm mentioning it here so the section in "Volume Mounts" makes sense below.
   # I'm importing the dags in my custom folder (located in "Host Path Volume")
   # to the dags folder that Airflow looks at.
   # The "Volume Mounts for DAGs" in the destination for where I import
   # the dags folder from my custom folder (myK3Spods_files/myAirflow/dags)
   # to 
   


# -------- Volume Mounts for DAGs -------- 
dags:
  volumeMounts:
   - name: host-dags
     mountPath: /opt/airflow/dags  # Must match AIRFLOW__CORE__DAGS_FOLDER

#  -------- Host Path Volume  -------- 
# We need to mount the folder in our ec2 instance
# into the K3S Kubernetes pod for airflow so airflow can access it

extraVolumes:
 - name: host-dags
   hostPath:
     path: /home/ec2-user/myK3Spods_files/myAirflow/dags  # Folder on the ec2-instance
     type: Directory


# -------- Limiting Resources -------- 

web:
  resources:
    requests:
      memory: 512Mi
      cpu: 200m
    limits:
      memory: 1Gi
      cpu: 500m
scheduler:
  resources:
    requests:
      memory: 512Mi
      cpu: 200m
    limits:
      memory: 1Gi
      cpu: 500m

triggerer:
  enabled: false

flower:
  enabled: false

redis:
  enabled: false

postgresql:
  enabled: true
  auth:
    postgresPassword: airflow
    username: airflow
    password: airflow
    database: airflow
  primary:
    resources:
      requests:
        memory: 256Mi
        cpu: 100m
      limits:
        memory: 512Mi
        cpu: 200m

dags:
  persistence:
    enabled: true
    size: 1Gi

logs:
  persistence:
    enabled: false
# Airflow Type Executor

executor: LocalExecutor


# -------- Airflow Environment Variables -------- 

env:
 - name: AIRFLOW__CORE__LOAD_EXAMPLES
   value: "False"


env:
 - name: AIRFLOW__CORE__DAGS_FOLDER
   value: {{.Values.dagFolderPath}} 

# -------- Volume Mounts for DAGs -------- 
dags:
  volumeMounts:
   - name: host-dags
     mountPath: {{.Values.dagFolderPath }}  # Must match AIRFLOW__CORE__DAGS_FOLDER

#  -------- Host Path Volume  -------- 
# We need to mount the folder in our ec2 instance
# into the K3S Kubernetes pod for airflow so airflow can access it

extraVolumes:
 - name: host-dags
   hostPath:
     path: /home/ec2-user/myK3Spods_files/myAirflow/dags  # Folder on the ec2-instance
     type: Directory


# -------- Limiting Resources -------- 

web:
  resources:
    requests:
      memory: 512Mi
      cpu: 200m
    limits:
      memory: 1Gi
      cpu: 500m
scheduler:
  resources:
    requests:
      memory: 512Mi
      cpu: 200m
    limits:
      memory: 1Gi
      cpu: 500m

triggerer:
  enabled: false

flower:
  enabled: false

redis:
  enabled: false

postgresql:
  enabled: true
  auth:
    postgresPassword: airflow
    username: airflow
    password: airflow
    database: airflow
  primary:
    resources:
      requests:
        memory: 256Mi
        cpu: 100m
      limits:
        memory: 512Mi
        cpu: 200m

dags:
  persistence:
    enabled: true
    size: 1Gi

logs:
  persistence:
    enabled: false